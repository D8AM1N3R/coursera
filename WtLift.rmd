Weight Lifting Exercise Project
========================================================

This project asked us to classify a weight lifting maneuver (measured by motion sensors attached to a belt, glove, arm band, and the weight).  The possible classes were labeled A through E, with A being the correct procedure and B-E being various incorrect procedures.  The paper (http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf) that the creators of the dataset published discusses their hypotheses, methodologies, and results--namely that they were able to achieve a 98.2% weighted accuracy using Random Forests and a 2.5s window of data capture.

First, we read the data into 2 datasets, WLEtrain and WLEtest:

```{r eval=FALSE}
root = 'C:/Users/Ben/Documents/R/ML Data/PML'
WLEtrain = read.csv(paste(root,'pml-training.csv',sep="/"))
WLEtest = read.csv(paste(root,'pml-testing.csv',sep="/"))
```

Next, after observing that many of the columns have an abundance of missing values (in the forms of NA and #Div/0!), we must drop the columns that are mostly missing.  For this we used a threshold of at most 10% NA/missing, but this value could be changed depending on one's confidence in imputing the missing values.

```{r eval=FALSE}
WLEtrain = WLEtrain[,8:160]
cols = names(WLEtrain)
n = 1
keep = c()
thresh = 0.1*dim(WLEtrain)[1]
for (i in cols){
  if (length(WLEtrain[[i]][is.na(WLEtrain[[i]])])+length(WLEtrain[[i]][WLEtrain[[i]]==""]) < thresh){
    keep[n]=i
    n = n+1
}
}
WLEtrain1 = WLEtrain[,c(keep)]
```
Next, since we likely do not need all 19,622 to build a random forest model, we will pare down our training dataset into a subset to build the model from and a subset to test against.

```{r eval=FALSE}
set.seed(1234)
library(caret)
ind = createDataPartition(y=WLEtrain1$classe,p=0.85,list=FALSE)
WLEtrain1_train = WLEtrain1[-ind,]
WLEtrain1_test = WLEtrain1[ind,]
```
One of the reasons that Random Forests are so popular is that they are able to handle correlated variables well whereas many other models require correlated variables to be either combined or eliminated.  Thus, even though we see 19 unique variable pairs that have a high absolute correlation (> 0.8), we chose to leave them in since the final model should pick the most useful of the correlated features.
```{r eval=FALSE}
modFit = train(classe~.,data=WLEtrain1_train,method="rf",prox=TRUE)
modFit
```
As you can see, the model reached optimal performance with 27 selected variables, achieving an Accuracy of 0.9486354.  Once we run this random forest model on the other 85% of the training set we see an Accuracy of 0.9595324:
```{r eval=FALSE}
pred = predict(modFit,WLEtrain1_test)
table(pred,WLEtrain1_test$classe)
```
<table border=1>
<tr>
<td> </td><td>A</td><td>B</td><td>C</td><td>D</td><td>E</td>
</tr>
<tr>
<td>A</td><td>4693</td><td>85</td><td>1</td><td>6</td><td>5</td>
</tr>
<tr>
<td>B</td><td>39</td><td>2998</td><td>59</td><td>0</td><td>28</td>
</tr>
<tr>
<td>C</td><td>4</td><td>142</td><td>2834</td><td>176</td><td>30</td>
</tr>
<tr>
<td>D</td><td>6</td><td>3</td><td>15</td><td>2547</td><td>45</td>
</tr>
<tr>
<td>E</td><td>1</td><td>0</td><td>0</td><td>5</td><td>2958</td>
</tr>
</table>
<br>

Since our random forest model was built with 25 bootsraps, we can be reasonably confident that the machine learning algorithm will perform well on the test data.  Now that we have our trained model, which performed well on our training data subset, we can apply it to the actual Testing dataset:
```{r eval=FALSE}
pred1 = predict(modFit,WLEtest1)
```

Once we loaded the results into the evaluation system we acheieved a 20 out of 20, 100% accuracy.
<br>
